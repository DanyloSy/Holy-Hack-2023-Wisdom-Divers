{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanyloSy/Holy-Hack-2023-Wisdom-Divers/blob/main/Copy_of_Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwOGutbdywsr",
        "outputId": "cc090a6b-5fa1-48fd-dbd7-bf601991fcda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4Gk5M14y0bG",
        "outputId": "dcf74c61-42c8-4847-f950-85fe9a5b5994"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.2-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 KB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.25.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (4.0.0)\n",
            "Collecting charset-normalizer<4.0,>=2.0\n",
            "  Downloading charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: multidict, frozenlist, charset-normalizer, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 charset-normalizer-3.1.0 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.2 yarl-1.8.2\n"
          ]
        }
      ],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCs1k4XOy6dB",
        "outputId": "f4652e7d-3b60-4b79-f3ca-48123fdab828"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the text of the business owner: I am a coffee house owner in the centre of Brussels. I did not have much clients in a last couple of weeks. Also, my coffee suppliers are unreliable\n",
            "Enter the text of the business owner: I am a coffee house owner in the centre of Brussels. I did not have much clients in a last couple of weeks. Also, my coffee suppliers are unreliable\n",
            "The most related keywords and their similarity scores are:\n",
            "Management : 50.0 % match\n",
            "Hiring : 33.33 % match\n",
            "Partnerships : 33.33 % match\n",
            "Leadership : 33.33 % match\n",
            "Accounting : 33.33 % match\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import *\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# define the list of keywords \n",
        "keywords = ['Cash flow', 'Marketing', 'Sales', 'Hiring', 'Staff retention', 'Product development', 'Competition', 'Financial management', 'Customer retention', 'Time management', 'Cybersecurity', 'Technology', 'Inventory management', 'E-commerce', 'Branding', 'Innovation', 'Pricing', 'Partnerships', 'Legal compliance', 'Taxes', 'Social media', 'Customer service', 'Growth', 'Risk management', 'Intellectual property', 'Supplier management', 'Quality control', 'Logistics', 'Distribution', 'Payment processing', 'Human resources', 'Leadership', 'Productivity', 'Regulations', 'Exporting', 'Importing', 'Reputation management', 'Employee benefits', 'Workplace safety', 'Employee training', 'Management', 'Accounting', 'Capital raising', 'Decision making', 'Information management', 'Outsourcing', 'Cash reserves', 'Employee engagement', 'Succession planning', 'Health and safety', 'Public relations', 'Local competition', 'Environmental regulations', 'Climate change', 'Data management', 'Communication', 'Supply chain management', 'Innovation management', 'Industry disruption', 'Business ethics', 'Market analysis', 'Change management', 'Budgeting', 'Customer insights', 'Product design', 'Investment management', 'Sales channels', 'Debt management', 'Customer acquisition', 'Customer loyalty', 'Product distribution', 'Business strategy', 'Market positioning', 'Business planning', 'Strategic partnerships', 'Recruitment', 'Workplace culture', 'Reputation', 'Business models', 'Staff morale', 'Personal finance', 'Cash reserves', 'Company values', 'Decision making', 'Risk assessment', 'Regulatory compliance', 'Managing expectations', 'Scalability', 'Resource allocation', 'Intellectual property rights', 'Financial planning', 'Market segmentation', 'Customer satisfaction', 'Product differentiation', 'Globalization', 'Product testing', 'Competition analysis', 'Business expansion', 'Business resilience', 'Product launch']\n",
        "\n",
        "# ask for the text of the business owner\n",
        "text = input(\"Enter the text of the business owner: \")\n",
        "\n",
        "# define a function to preprocess the text\n",
        "def preprocess_text(text):\n",
        "    # convert the text to lowercase\n",
        "    text = text.lower()\n",
        "    # tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "    # remove stop words\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    # lemmatize the words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return words\n",
        "\n",
        "# define a function to calculate the keyword similarity\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# define a function to calculate the keyword similarity\n",
        "def get_keyword_similarity(text, keywords):\n",
        "    # preprocess the text\n",
        "    words = preprocess_text(text)\n",
        "    # initialize the keyword similarity dictionary\n",
        "    keyword_similarity = {}\n",
        "    # calculate the similarity between each keyword and the text\n",
        "    for keyword in keywords:\n",
        "        keyword_synsets = wordnet.synsets(keyword)\n",
        "        text_synsets = [wordnet.synset(synset.name()) for word in words for synset in wordnet.synsets(word)]\n",
        "        similarity_scores = []\n",
        "        for synset in keyword_synsets:\n",
        "            synset_scores = [synset.path_similarity(text_synset) for text_synset in text_synsets if synset.path_similarity(text_synset) is not None]\n",
        "            if synset_scores:\n",
        "                similarity_scores.append(max(synset_scores))\n",
        "        keyword_similarity[keyword] = max(similarity_scores) if similarity_scores else 0\n",
        "    return keyword_similarity\n",
        "\n",
        "# define a function to summarize the text\n",
        "def summarize_text(text, keywords):\n",
        "    # calculate the keyword similarity\n",
        "    keyword_similarity = get_keyword_similarity(text, keywords)\n",
        "    # sort the keywords by similarity score in descending order\n",
        "    sorted_keywords = sorted(keyword_similarity.items(), key=lambda x: x[1], reverse=True)\n",
        "    # get the top 5 keywords and their similarity scores\n",
        "    top_keywords = [(keyword[0], round(keyword[1] * 100, 2)) for keyword in sorted_keywords[:5]]\n",
        "    return top_keywords\n",
        "\n",
        "# ask for the text of the business owner\n",
        "text = input(\"Enter the text of the business owner: \")\n",
        "\n",
        "# summarize the text and list the most related keywords\n",
        "keywords_list = summarize_text(text, keywords)\n",
        "print(\"The most related keywords and their similarity scores are:\")\n",
        "for keyword, similarity_score in keywords_list:\n",
        "    print(keyword, \":\", similarity_score, \"% match\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt_1PrBKy6fL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "df = pd.read_csv(\"cases.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gPtKOYqy-dW"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace(\".\", \"\")\n",
        "    text = text.replace(\",\", \"\")\n",
        "    text = text.replace(\"?\", \"\")\n",
        "    text = text.replace(\"!\", \"\")\n",
        "    text = text.replace(\"-\", \" \")\n",
        "    return text\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(preprocessor=preprocess_text)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df[\"Business Problem\"] + \" \" + df[\"Solution\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_pxJCX0zAMx"
      },
      "outputs": [],
      "source": [
        "def get_relevant_problem_and_solution(keywords):\n",
        "    keywords = preprocess_text(keywords)\n",
        "    keywords_vector = tfidf_vectorizer.transform([keywords])\n",
        "    similarities = cosine_similarity(keywords_vector, tfidf_matrix).flatten()\n",
        "    index = similarities.argsort()[::-1][0]\n",
        "    return df.iloc[index][\"Business Problem\"], df.iloc[index][\"Solution\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YC7Sc0XzB3u",
        "outputId": "5d61eb28-a9ba-49ea-eb03-8fb04943aad2"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: SmartCrops, a startup focused on developing cutting-edge technologies for sustainable agriculture, is struggling to gain traction in the highly competitive AgriTech industry.\n",
            "Bot: Develop a targeted marketing strategy - SmartCrops can build awareness and generate interest in its technology by implementing a targeted marketing strategy that focuses on its unique value proposition and differentiators. This could include: Industry events and conferences - By participating in industry events and conferences, SmartCrops can network with potential customers and partners, showcase its technology, and generate leads. Digital marketing campaigns - By leveraging social media, email marketing, and other digital channels, SmartCrops can reach a broader audience and build awareness of its brand and products. Thought leadership content - By producing thought leadership content such as white papers, webinars, and blog posts, SmartCrops can establish itself as a leader in sustainable agriculture technology and build credibility with potential customers and partners. Secure strategic partnerships - SmartCrops can accelerate its path to market by partnering with established companies and organizations that have complementary products and services, as well as strong customer bases. This could include: Agricultural equipment manufacturers - By partnering with agricultural equipment manufacturers, SmartCrops can integrate its technology into existing equipment and offer a complete solution to customers. Sustainable agriculture organizations - By partnering with sustainable agriculture organizations, SmartCrops can gain access to a network of potential customers and partners, as well as leverage their expertise and resources. Secure funding - SmartCrops can accelerate its technology development and commercialization efforts by securing funding from investors who are interested in sustainable agriculture and cutting-edge technology. This could include: Venture capitalists - By pitching to venture capitalists who specialize in sustainable agriculture and technology, SmartCrops can secure funding to take its technology to market. Grants and other funding programs - By applying for grants and other funding programs that support sustainable agriculture and innovation, SmartCrops can access non-dilutive funding to advance its technology.\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    keywords = input(\"User: \")\n",
        "    problem, solution = get_relevant_problem_and_solution(keywords)\n",
        "    print(\"Bot: \" + problem)\n",
        "    print(\"Bot: \" + solution)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZ0eZ/vLygg9JJNKRbNTl7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}